---
title: "project_3"
output: html_document
date: "2022-11-11"
---

```{r}
library(text2vec)
library(glmnet)
library(pROC)
library(slam)
library(dplyr)
```

```{r}
all_data = read.table("alldata.tsv",
                   stringsAsFactors = FALSE,
                   header = TRUE)
all_data$review = gsub('<.*?>', ' ', all_data$review)

stop_words = c("i", "me", "my", "myself", 
               "we", "our", "ours", "ourselves", 
               "you", "your", "yours", 
               "their", "they", "his", "her", 
               "she", "he", "a", "an", "and",
               "is", "was", "are", "were", 
               "him", "himself", "has", "have", 
               "it", "its", "the", "us")
it_all_data = itoken(all_data$review,
                  preprocessor = tolower, 
                  tokenizer = word_tokenizer)
tmp.vocab = create_vocabulary(it_all_data, 
                              stopwords = stop_words, 
                              ngram = c(1L,4L))
tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10,
                             doc_proportion_max = 0.5,
                             doc_proportion_min = 0.001)
dtm_all_data  = create_dtm(it_all_data, vocab_vectorizer(tmp.vocab))

v.size = dim(dtm_all_data)[2]

y_all = all_data$sentiment

summ = matrix(0, nrow=v.size, ncol=4)
summ[,1] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix(dtm_all_data[y_all==1, ]), mean)
summ[,2] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix(dtm_all_data[y_all==1, ]), var)
summ[,3] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix(dtm_all_data[y_all==0, ]), mean)
summ[,4] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix(dtm_all_data[y_all==0, ]), var)

n1 = sum(y_all); 
n = length(y_all)
n0 = n - n1

myp = (summ[,1] - summ[,3])/
  sqrt(summ[,2]/n1 + summ[,4]/n0)

words = colnames(dtm_all_data)
id = order(abs(myp), decreasing=TRUE)[1:2000]
pos.list = words[id[myp[id]>0]]
neg.list = words[id[myp[id]<0]]

id1 = which(summ[, 2] == 0) # same as: which(summ[id0, 1] != 0)
id0 = which(summ[, 4] == 0) #same as: which(summ[id1, 3] != 0)

myvocab_start = union(id,id1)
myvocab_start = union(myvocab_start,id0)
myvocab_start = words[myvocab_start]

vectorizer = vocab_vectorizer(create_vocabulary(myvocab_start, 
                                                  ngram = c(1L, 2L)))

set.seed(1852) 
tmpfit = glmnet(x = dtm_all_data,
                 y = all_data$sentiment,
                 alpha = 1,
                 family='binomial')

myvocab = colnames(dtm_all_data)[which(tmpfit$beta[, 36] != 0)]

write.table(myvocab, file = "myvocab.txt",
            quote = FALSE,
            row.names = FALSE,
            col.names = FALSE,
            sep = "\n")

```


```{r}
vectorizer = vocab_vectorizer(create_vocabulary(myvocab,
                                                ngram = c(1L, 2L)))

for (j in 1:5){
  setwd(paste("split_", j, sep=""))
  
  test <- read.table("test.tsv",
                     stringsAsFactors = FALSE,
                     header = TRUE)
  
  test_y <- read.table("test_y.tsv",
                     stringsAsFactors = FALSE,
                     header = TRUE)
  
  
  train = read.table("train.tsv",
                     stringsAsFactors = FALSE,
                     header = TRUE)
  train$review = gsub('<.*?>', ' ', train$review)
  
  it_train = itoken(train$review,
                    preprocessor = tolower, 
                    tokenizer = word_tokenizer)
  
  dtm_train = create_dtm(it_train, vectorizer)
  
  it_test = itoken(test$review,
                      preprocessor = tolower,
                      tokenizer = word_tokenizer)
  
  dtm_test = create_dtm(it_test, vectorizer)
  
  fit <- cv.glmnet(y = train$sentiment, x = dtm_train, family = 'binomial', alpha = 0)
  
  
  y_probs <- predict(fit, dtm_test, type="response", s = fit$lambda.1se)
  
  
  logist_auc = auc(test_y$sentiment, y_probs)
  print(logist_auc)
  setwd("..")
}

```